but {disfmarker} but , uh , probably , if we had to pick something {pause} that we would talk on for ten minutes or so while they 're coming here . Or I guess it would be , you think , reorganization status ,
I mean , I think , Chuck was the one who added out the agenda item .
I don't really have anything to say other than that we still haven't done it .
And I {disfmarker} and I think a crucial part of that is the idea of {disfmarker} of not wanting to do it until right before the next level zero back - up so that there won't be huge number of {disfmarker} of added ,
So , naming conventions and things like that , that I 've been trying to keep actually up to date .
And I 've been sharing them with U - d UW folks also .
So , we , uh {disfmarker} we did another version of the beeps , where we separated each beeps with a spoken digit .
Chuck came up here and recorded some di himself speaking some digits ,
Well , maybe uh , since that {disfmarker} that was a pretty short one , maybe we should talk about the IBM transcription status .
And we have done that on the {pause} automatic segmentations .
Um , I think they 'll have a b easier time keeping track of where they are in the file .
We just sent it to IBM .
And the main thing will be if we can align what they give us with what we sent them .
I {disfmarker} I hire {disfmarker} I 've hired two extra people already , expect to hire two more .
She 's also checking through and mar and {disfmarker} {vocalsound} and monitoring , um , the transcription of another transcriber .
which are now being edited by my head transcriber , {vocalsound} in terms of spelling errors and all that .
And , I 've moved on now to what I 'm calling set three .
I sort of thought if I do it in sets {disfmarker} groups of five , then I can have , like , sort of a {disfmarker} a parallel processing through {disfmarker} through the {disfmarker} the current .
And {disfmarker} and you indicated to me that we have a g a goal now , {vocalsound} for the {disfmarker} for the , um , {nonvocalsound} {vocalsound} the , uh , DARPA demo , of twenty hours .
So , I 'm gonna go up to twenty hours , be sure that everything gets processed , and released , and {disfmarker} {pause} {comment} and that 's {disfmarker} that 's what my goal is .
But I guess the other thing is that , um , that {disfmarker} that 's kinda twenty hours ASAP because the longer before the demo we actually have the twenty hours , the more time it 'll be for people to actually do cool things with it .
Yeah , I mean , I guess the {disfmarker} So the difference if {disfmarker} if , um , if the IBM stuff works out , the difference in the job would be that they p primarily would be checking through things that were already done by someone else ?
Well , {vocalsound} I realize that , um , w i we we 're using the pre - segmented version ,
and , um , the pre - segmented version is extremely useful ,
and wouldn't it be , useful also to have the visual representation of those segments ?
And so I 've {disfmarker} {pause} uh , {pause} I , uh , uh , I 've {comment} trained the new one {disfmarker} uh , the new the newest one , {vocalsound} to , um , {vocalsound} use the visual from the channel that is gonna be transcribed at any given time .
Because what happens then , is you scan across the signal and once in a while you 'll find a blip that didn't show up in the pre - segmentation .
And , I think that we 're gonna end up with , uh {pause} better coverage of the backchannels ,
but at the same time we 're benefitting tremendously from the pre - segmentation
so , I was just wondering what people thought about how automated can we make the process of finding where the people read the digits , doing a forced alignment , and doing the timing .
Well , forced alignment would be one thing .
What about just actually doing recognition ?
I was just asking , just out of curiosity , if {disfmarker} if with , uh {disfmarker} uh , the SRI recognizer getting one percent word error ,
uh , would we {disfmarker} would we do {pause} better {disfmarker} ?
So , if you do a forced alignment but the force but the {disfmarker} but the transcription you have is wrong because they actually made mistakes , uh , or {vocalsound} false starts , it 's {disfmarker} it 's much less c {vocalsound} it 's {pause} much less common than one percent ?
Well , I guess {disfmarker} yeah , I guess if we segmented it , we could get one percent on digits .
I 'm not saying it should be one way or the other , but it 's {disfmarker} If {disfmarker}
Hire some people , or use the transcribers to do it .
Um , or we could try some automated methods .
And my {disfmarker} my tendency right now is , well , if IBM comes back with this meeting and the transcript is good , just let them do it .
and {disfmarker} {vocalsound} and {disfmarker} and , uh , one of the obvious things that occur to us was that we 're {disfmarker} since we now have Thilo 's segmenter and it works , you know , amazingly well , {vocalsound} um , we should actually basically re - evaluate the recognition , um , results using {disfmarker} you know , without cheating on the segmentations .
No , actually , um , NIST has , um m a fairly sophisticated scoring program {vocalsound} that you can give a , um {disfmarker} {vocalsound} a time ,
The references for {disfmarker} for {pause} those segments ?
uh {disfmarker} You know , you basically just give two {pause} time - marked sequences of words , and it computes the um {disfmarker} the , {comment} uh {disfmarker} {comment} you know , the {disfmarker} the {disfmarker} th
So , it {disfmarker} we just {disfmarker} and we use that actually in Hub - five to do the scoring .
Um . So what we 've been using so far was sort of a {pause} simplified version of the scoring .
It does time - constrained word - alignment .
That Thilo wanted to use {pause} the recognizer alignments to train up his , um , speech detector .
Um , so that we could use , uh {disfmarker} you know there wouldn't be so much hand {vocalsound} labelling needed to , uh {disfmarker} to generate training data for {disfmarker} for the speech detector .
and the , uh {disfmarker} Porzel {disfmarker} and the , uh , SmartKom group are collecting some dialogues .
Basically they have one person sitting in here , looking at a picture , and a wizard sitting in another room somewhere .
And , uh , they 're doing a travel task .
but it starts where the wizard is pretending to be a computer and it goes through a , uh , {vocalsound} speech generation system .
Should this be part of the corpus or not ?
And my attitude was yes , because there might be people who are using this corpus for {pause} acoustics , as opposed to just for language .
We simulate a computer breakdown halfway through the session , and so then after that , the person 's told that they 're now talking to a , uh {disfmarker} to a human .
But of course they don't know that it 's the same person both times .
and I said , " well that 's silly , if {disfmarker} if we 're gonna try to do it for a corpus , there might be people who are interested in acoustics . "
I {disfmarker} I would not say it was part of the meetings corpus .
So it 's {disfmarker} It {disfmarker} it {disfmarker} I guess it {disfmarker} the {disfmarker} begs the question of what is the meeting corpus .
I think it 's {disfmarker} I {disfmarker} I think {disfmarker} I th think the idea of two or more people conversing with one another is key .
Well , this has two or more people conversing with each other .
we give everyone who 's involved as their own user ID , give it session I Ds , {vocalsound} let all the tools that handle Meeting Recorder handle it , or do we wanna special case it ?
Well , it {disfmarker} it makes sense to handle it with the same infrastructure , since we don't want to duplicate things unnecessarily .
But as far as distributing it , we shouldn't label it as part of this meeting corpus .
And {disfmarker} and those {disfmarker} and this sounds like it 's more of an experimental setup .
Because we have , like , meetings that have a reason .
It 's scenario - based , it 's {disfmarker} it 's human - computer interface {disfmarker} {vocalsound} it 's really pretty different .
It 's just that it 's , you know , different directory , it 's called something different , it 's {disfmarker}
and just simply in the file you mark somewhere that this is this type of interaction , rather than another type of interaction .
Well , I don I wouldn't call reading digits " meetings " .
Well , but {disfmarker} but , {vocalsound} I put it under the same directory tree .
I mean , I don't care what directory tree you have it under .
So , once everything gets converted over to the disks we 're supposed to be using we 'll be probably , uh , seventy - five percent .
We 're about {disfmarker} we 're about half {disfmarker} halfway through our disk right now .
I 'm much more concerned about the backed - up .
But , uh {disfmarker} I {disfmarker} I {disfmarker} you don't want to per p have your only copy on a media that fails .
What about putting the stuff on , like , C - CD - ROM or DVD or something ?
So {disfmarker} so how about putting them on that plus , like on a {disfmarker} on {disfmarker} on DAT or some other medium that isn't risky ?
I mean , when I say two or three years what I 'm saying is that I have had disks which are gone in a year .
ICSI already has a perfectly good tape system and it 's more reliable .
So for archiving , we 'll just use tape .
But even without that , the back - up system is becoming saturated .
But {disfmarker} but this back - up system is smart enough to figure out that something hasn't changed and doesn't need to be {pause} backed - up again .
Well , but you can have it NW archive to {disfmarker} you can have , {vocalsound} uh , a non - backed - up disk NW archived ,
N I 'm successfully , uh , increasing the error rate .
So , I mean I 'm just playing with , um , the number of Gaussians that we use in the {disfmarker} the recognizer , and {disfmarker}
Well , you have to sa you have to {pause} tell people that you 're {disfmarker} you 're doing {disfmarker} you 're trying the tandem features .
A and I 'm still tinkering with the PLP features .
That was {disfmarker} that was before I tried it on the females .
we had reached the point where , {comment} um , on the male portion of the {pause} development set , the , um {disfmarker} or one of the development sets , I should say {disfmarker} {vocalsound} the , um {disfmarker} the male error rate with , uh , ICSI PLP features was pretty much identical with , uh , SRI features .
Oh , and plus the {disfmarker} the vocal tract {pause} length normalization didn't {disfmarker} actually made things worse .
Um , and the test data is CallHome and Switchboard .
So something 's really seriously wrong .
So {disfmarker} but you see , now , between {disfmarker} between the males and the females , there 's certainly a much bigger difference in the scaling range , than there is , say , just within the males .
d so the one thing that I then tried was to put in the low - pass filter , which we have in the {disfmarker}
Although , you know , normally , I mean , the channel goes to four {disfmarker} four thousand .
Um {pause} and it didn't hurt on the males either .
Oh , and suddenly , also the v the vocal tract length normalization only in the test se on the test data .
maybe between one and two percent , um , for the females .
Well , you can try each one on a cross - validation set ,
Well , he was {disfmarker} he 's {disfmarker} it looked like the probabil at one point he was looking at the probabilities he was getting out {disfmarker} at the likelihoods he was getting out of PLP versus mel cepstrum , and they looked pretty different ,
But , you 're only talking about a percent or two .
So , for the PLP features we use the triangular filter shapes .
And for the {disfmarker} in the SRI front - end we use the trapezoidal one .
So , is there something quick about Absinthe {pause} that you {disfmarker} ?
and got {vocalsound} {vocalsound} a speedup roughly proportional to the number of processors times the clock cycle .
But the {disfmarker} what it means is that it 's likely that for net training and forward passes , we 'll {disfmarker} Absinthe will be a good machine .
Especially if we get a few more processors and upgrade the processors .
So I guess the other thing that we were gonna talk about is {disfmarker} is , uh , demo .
And , um , so , these are the demos for the {pause} uh , July , uh , meeting {pause} and , um {disfmarker} DARPA mee
but maybe , uh {disfmarker} maybe we 'll just put that off for now , given that {disfmarker}
But I think maybe we should have a {disfmarker} a sub - meeting ,
I think , uh , probably , uh , Adam and {disfmarker} and , uh , Chuck and me should talk about {disfmarker} should get together and talk about that sometime soon .
that the {disfmarker} that the , um , {vocalsound} PLP , and {disfmarker} and the reason PLP has been advantageous in , uh , slightly noisy situations is because , {vocalsound} PLP does the smoothing at the end by an auto - regressive model ,
We 've always viewed it , anyway , as the major difference between the two , is actually in the smoothing ,
We could let IBM transcribe it .
So , most {disfmarker} most Hub - five systems actually band - limit the {disfmarker} uh , at about , uh , thirty - seven hundred , um , hertz .
We 're looking at the discrepancy between the SRI system and the SRI system when trained with ICSI features .
No , but with Baum - Welch , there shouldn't be an over - fitting issue , really .
Or maybe {disfmarker} or maybe you 're doing one too many .
