Um , so , uh , let 's see , we were having a discussion the other day , maybe we should bring that up , about uh , the nature of the data that we are collecting .
um , and was thinking of this mostly just so that we could do research on this data um , since we 'll have a new {disfmarker} this new student di does wanna work with us ,
uh @ @ that uh , we should have a fair amount of data that is um , collected for the same meeting , so that we can ,
he 's {disfmarker} comes from a signal - processing background ,
cuz he 's very interested in higher level things , like language , and disfluencies and all kinds of eb maybe prosody ,
um , but I 'd rather try to get more regular meetings of types that we know about , and hear , then sort of a mish - mosh of a bunch of one {disfmarker} one - time {disfmarker}
So . Then I was um , talking to Morgan about some {pause} new proposed work in this area , sort of a separate issue from what the student would be working on where I was thinking of doing some kind of summarization of meetings or trying to find cues in both the utterances and in the utterance patterns ,
like in numbers of overlaps and amount of speech ,
sort of raw cues from the interaction that can be measured from the signals and from the diff different microphones that point to sort of hot spots in the meeting , or things where stuff is going on that might be important for someone who didn't attend to {pause} listen to .
And in that uh , regard , I thought we definitely w will need {disfmarker} it 'd b it 'd be nice for us to have a bunch of data from a few different domains , or a few different kinds of meetings .
like the front - end meeting {pause} and maybe a networking {pause} group meeting .
So {disfmarker} So if that were the case then I think we 'd have enough .
but {disfmarker} rather we should have different meetings by the same group but hopefully that have different summaries .
but also data where we hold some parameters constant or fairly similar ,
like a meeting about of people doing a certain kind of work where at least half the participants each time are the same .
Uh , for other kinds of research , particularly the acoustic oriented research , I actually feel the opposite need .
I 'd like to have many different speakers .
So , um I think I would also very much like us to have a fair amount of really random scattered meetings , of somebody coming down from campus , and {disfmarker} and uh ,
but if we only get one or two from each group , that still could be useful acoustically just because we 'd have close and distant microphones with different people .
It has to be a {disfmarker} a pre - existing meeting , {pause} like a meeting that would otherwise happen anyway .
So I was {disfmarker} I was thinking more in terms of talking to professors uh , and {disfmarker} and {disfmarker} and uh , senior uh , uh , d and uh , doctoral students who are leading projects and offering to them that they have their {disfmarker} hold their meeting down here .
Well , I think that , um {disfmarker} I think that the only thing we should say in the advertisement is that the meeting should be held in English .
If you have people who are using English as a {disfmarker} as an interlanguage because they {disfmarker} they don't {disfmarker} uh , they can't speak in their native languages and {disfmarker} but their interlanguage isn't really a match to any existing , uh , language model ,
And I 'm not objecting to accents .
I 'm {disfmarker} I {disfmarker} what I think is that why not have the corpus , since it 's so expensive to put together , uh , useful for the widest range of {disfmarker} of central corp things that people generally use corpora for and which are , you know , used in computational linguistics .
I mean , it {disfmarker} it {disfmarker} I think that if we 're aiming at {disfmarker} at uh , groups of graduate students and professors and so forth who are talking about things together , and it 's from the Berkeley campus , probably most of it will be OK ,
And my point in m in my note to Liz was I think that undergrads are an iff iffy population .
Well , Morgan , you were mentioning that Mari may not use the k equipment from IBM if they found something else , cuz there 's a {disfmarker}
Cuz I mean , one remote possibility is that if we st if we inherited that equipment , if she weren't using it , could we set up a room in the linguistics department ?
And {disfmarker} and I mean , there {disfmarker} there may be a lot more {disfmarker} or {disfmarker} or in psych , or in comp wherever , in another building where we could um , record people there .
Well , the other thing {disfmarker} Yeah , I mean the other thing that I was hoping to do in the first place was to turn it into some kind of portable thing so you could wheel it around .
Uh , i We realized in discussion that the other thing is , what about this business of distant and close microphones ?
I mean , we really wanna have a substantial amount recorded this way ,
But {pause} what about {disfmarker} For th for these issues of summarization , a lot of these higher level things you don't really need the distant microphone .
And you don't really need the close microphone , you mean .
you know , each person who 's interested in {disfmarker} I mean , we have a cou we have a bunch of different , um , slants and perspectives on what it 's useful for , um , they need to be taking charge of making sure they 're getting enough of the kind of data that they want .
And {disfmarker} So in my case , um , I think there w there is enough data for some kinds of projects and not enough for others .
And other people need to do that for themselves , uh , h or at least discuss it so that we can find some optimal {disfmarker}
So I {pause} do think that long term you should always try to satisfy the greatest number of {disfmarker} of interests and have this parallel information , which is really what makes this corpus powerful .
Uh but I {disfmarker} I think that the uh {vocalsound} i We can't really underestimate the difficulty {disfmarker} shouldn't really u underestimate the difficulty of getting a setup like this up .
If you 're talking about something simple , where you throw away a lot of these dimensions , then you can do that right away .
I think the first priority should be to pry {comment} to get {disfmarker} try to get people to come here .
But the issue is you definitely wanna make sure that the kind of group you 're getting is the right group
Um , I had a {disfmarker} I spoke with some people up at Haas Business School who volunteered .
Should I pursue that ?
Oh , definitely , yeah .
I 'd love to get people that are not linguists or engineers , cuz these are both weird {disfmarker}
The o the o the other {disfmarker} The other thing is , uh , that we {disfmarker} we talked about is give to them {disfmarker} uh , burn an extra CD - ROM .
We could burn it after it 's been cleared with the transcript stage .
So , after the transcript screening phase .
and {vocalsound} then um , I guess another topic would be {vocalsound} where are we in the whole disk resources {pause} question
We are getting , uh , another disk rack and {disfmarker} and four thirty - six gigabyte disks .
Uh {pause} so {pause} uh {pause} but that 's not gonna happen instantaneously .
Uh , OK , @ @ {comment} So , uh , then I guess th the last thing I 'd had on my {disfmarker} my agenda was just to hear {disfmarker} hear an update on {vocalsound} what {disfmarker} what Jose has been doing ,
I have , eh , {vocalsound} The result of my work during the last days .
But for me , eh is interesting because , eh , eh , here 's i is the demonstration of the overlap , eh , {pause} problem .
Eh , this information is very {disfmarker} very useful .
Because {vocalsound} you have the {disfmarker} the {disfmarker} the distribution , now .
It 's a real problem , {comment} a frequently problem {comment} uh , because you have overlapping zones eh , eh , eh , all the time .
Throughout the meeting .
Eh , by a moment I have , eh , nnn , the , eh , {pause} n I {disfmarker} I did a mark of all the overlapped zones in the meeting recording ,
Heh ? That 's eh , yet b b Yeah , by {disfmarker} b b by hand {disfmarker} by hand because , eh , {vocalsound} eh {disfmarker} " Why . "
but , eh , my idea is , eh , is very interesting to {disfmarker} to work {pause} in {disfmarker} in the line of , eh , automatic segmenter .
And so are you planning to do that or have you done that already ?
No , I {disfmarker} I {disfmarker} plan to do that .
Now , {vocalsound} eh , I need ehm , {vocalsound} to detect eh all the overlapping zones exactly .
Eh , um , {vocalsound} {vocalsound} eh {disfmarker} This information eh , with eh , exactly time marks eh , for the overlapping zones {vocalsound} eh {disfmarker} overlapping zone , and eh , a speaker {disfmarker} a {disfmarker} a pure speech eh , eh , speaker zone .
I mean , eh zones eh of eh speech of eh , one speaker without any {disfmarker} any eh , noise eh , any {disfmarker} any acoustic event eh that eh , eh , w eh , is not eh , speech , real speech .
for that , because my {disfmarker} my idea is to {disfmarker} to study the nnn {disfmarker} the {disfmarker} {vocalsound} the set of parameters eh , what , eh , are more m more discriminant to eh , classify .
the overlapping zones in cooperation with the speech {pause} eh zones .
The idea is {pause} to eh {disfmarker} to use {disfmarker} eh , I 'm not sure to {disfmarker} eh yet , but eh my idea is to use a {disfmarker} a cluster {pause} {vocalsound} eh algorithm or , nnn , a person strong in neural net algorithm to eh {disfmarker} to eh study
what is the , eh , the property of the different feat eh feature , eh , to classify eh speech and overlapping eh speech .
And my control set eh , will be the eh , silence , silence without eh , any {disfmarker} any noise .
eh , eh , event eh , which , eh , eh , has , eh eh , a hard effect of distorti spectral distortion in the {disfmarker} in the eh {pause} speech .
I have , eh , {pause} two hundred and thirty , more or less , overlapping zones , and is similar to {disfmarker} to this information ,
because I {disfmarker} I want to put , eh , for eh , each frame a label {pause} indicating . It 's a sup supervised and , eh , hierarchical clustering process .
I {disfmarker} I {disfmarker} I put , eh , eh , for each frame {nonvocalsound} a label indicating what is th the type , what is the class , eh , which it belong .
a I {disfmarker} I {disfmarker} I ha I h I {disfmarker} I put the mark by hand ,
because , eh , {vocalsound} my idea is , eh , in {disfmarker} in the first session , I need , eh , {pause} I {disfmarker} I need , eh , to be sure that the information eh , that , eh , I {disfmarker} I will cluster , is {disfmarker} is right . Because , eh , eh , if not , eh , I will {disfmarker} I will , eh , return to the speech file to analyze eh , what is the problems ,
So you 're ignoring overlapping events unless they 're speech with speech .
Yeah .
In the {disfmarker} in the future , the {disfmarker} the idea is to {disfmarker} to extend {pause} the class ,
But steady - state noises are part of the background .
Cuz you 're calling {disfmarker} what you 're calling " event " is somebody coughing {vocalsound} or clicking , or rustling paper , or hitting something , which are impulsive noises .
Right , it 's {disfmarker} I mean , it 's {disfmarker} " Background " might be {disfmarker} might be a better word than " silence " .
And eh I am going {pause} to prepare a test bed , eh , well , eh , a {disfmarker} a set of {pause} feature structure eh , eh , models .
so {disfmarker} so {disfmarker} on {disfmarker} because I have a pitch extractor yet .
And in a first eh , nnn , step in the investi in the research in eh , my idea is try to , eh , to prove , what is the performance of the difference parameter , eh {pause} to classify {pause} the different , eh , what is the {disfmarker} the {disfmarker} the {disfmarker} the front - end approach to classify eh , the different , eh , frames of each class {pause} eh and what is the {disfmarker} the , nnn , nnn , nnn , eh , what is the , the error {pause} eh , of the data
Supervised clustering .
and the second {pause} is try to {disfmarker} eh , to use {pause} some ideas eh , similar to the linear discriminant analysis .
Eh , the {disfmarker} the {disfmarker} the classifier is {disfmarker} nnn by the moment is eh {disfmarker} is eh , similar , nnn , that the classifier used eh , in a quantifier {disfmarker} vectorial quantifier is eh , used to {disfmarker} to eh , some distance {pause} to {disfmarker} to put eh , a vector eh , in {disfmarker} in a class different .
A another possibility it to use eh a netw netw a neural network .
but the thing is here he 's {disfmarker} he 's not {disfmarker} he 's not like he has one you know , a bunch of very distinct variables , like pitch and this {disfmarker} he 's talking about , like , a all these cepstral coefficients , and so forth ,
in which case a a any reasonable classifier is gonna be a mess , and it 's gonna be hard to figure out what {disfmarker} what uh {disfmarker}
I would take just a few features . Instead of taking all the MFCC 's , or all the PLP 's or whatever , I would just take a couple .
Like {disfmarker} like C - one , C - two , something like that , so that you can visualize it .
and look at these different examples and look at scatter plots .
OK , so before you do {disfmarker} build up any kind of fancy classifiers , just take a look in two dimensions , at how these things are split apart .
So if you 're just looking at a frame and a time , you don't know anything about , you know , the structure of it over time , and so you may wanna build @ @ {disfmarker} build a Markov model of some sort uh , or {disfmarker} or else have features that really are based on um on {disfmarker} on some bigger chunk of time .
But don't uh anyway , this is my suggestion , is don't just , you know , throw in twenty features at it , the deltas , and the delta del and all that into some classifier , even {disfmarker} even if it 's K - nearest - neighbors , you still won't know
look at {disfmarker} at som some picture that shows you , " Here 's {disfmarker} These things uh , uh are {disfmarker} offer some separation . " {vocalsound} And , uh , in LPC , uh , the thing to particularly look at is , I think {disfmarker} is something {vocalsound} like , uh , the residual {disfmarker}
But eh eh I {disfmarker} I understand that you {disfmarker} your objective is {pause} to eh classify , to know that eh that zone {pause} not is only {comment} a new zone in the {disfmarker} in the file , that eh you have eh , but you have to {disfmarker} to {disfmarker} to know that this is overlap zone .
because in the future you will eh try to {disfmarker} to process that zone with a non - regular eh eh speech recognizer model , I suppose .
you {disfmarker} you will pretend {comment} to {disfmarker} to {disfmarker} to process the overlapping z eh zone with another kind of algorithm
because it 's very difficult to {disfmarker} to {disfmarker} to obtain the transcription {pause} from eh using eh eh a regular , normal speech recognizer .
A model to detect more acc the mor most accurately possible that is p uh , will be possible the , eh {disfmarker} the mark , the change
and another {disfmarker} another model will @ @ {pause} or several models , to try s but {disfmarker} eh several model eh robust models , sample models to try to classify the difference class .
Because what we had before for {disfmarker} for uh , speaker change detection did not include these overlaps .
So the first thing is for you to {disfmarker} to build up something that will detect the overlaps .
so if you look at {disfmarker} {vocalsound} Suppose you look at first and second - order cepstral coefficients for some one of these kinds of things and you find that the first - order is much more effective than the second , {vocalsound} and then you look at the third and there 's not {disfmarker} and not too much there , {vocalsound} you may just take first and second - order cepstral coefficients ,
And with LPC , I think LPC per se isn't gonna tell you much more than {disfmarker} than {disfmarker} than the other , maybe .
Uh , and uh on the other hand , the LPC residual , the energy in the LPC residual , {vocalsound} will say how well , uh {vocalsound} the low - order LPC {vocalsound} model 's fitting it , which should be {vocalsound} pretty poorly for two two or more {vocalsound} people speaking at the same time , and it should be pretty well , for w for {disfmarker} for one .
And then you can do decision trees or whatever to see how they combine .
And so , if you can get {disfmarker} @ @ {comment} Uh again , my prescription would be that you would , with a mixed signal , you would take a collection of possible uh , features {vocalsound} look at them , look at how these different classes that you 've marked , separate themselves , {comment} {vocalsound} and then collect , uh in pairs , {vocalsound} and then collect ten of them or something , and then proceed {vocalsound} with a bigger classifier .
And then if you can get that to work well , then you go to the other signal .
but eh what is the {disfmarker} the {disfmarker} the relation of eh {disfmarker} of the {vocalsound} performance when eh you use eh the , eh eh speech file the PDA speech files .
Well , that {disfmarker} that {disfmarker} that 's another reason why very simple features , things like energy , and things {disfmarker} things like harmonicity , and {vocalsound} residual energy are uh , yeah are {disfmarker} are better to use than very complex ones because they 'll be more reliable .
because can't you , couldn't you like use beam - forming or something to detect speaker overlaps ?
I think {disfmarker} {vocalsound} I {disfmarker} I think {disfmarker} I think it 's {disfmarker} it 's {disfmarker} it 's a {disfmarker} it 's an additional interesting question .
Well , if you used the array , rather than the signal from just one .
That 's {pause} a good thing to consider .
So if there 's a distributed beam pattern , then it looks more like it 's {disfmarker} it 's uh , multiple people .
And if there are multiple people talking , you 'll see two peaks .
It 's spread out .
But {disfmarker} {vocalsound} but {disfmarker} but the thing is , uh , one of the {disfmarker} at least one of the things I was hoping to get at with this is what can we do with what we think would be the normal situation if some people get together and one of them has a PDA .
but {disfmarker} you know if you can instrument a room , this is really minor league compared with what some people are doing , right ?
But , the reason why I haven't focused on that as the fir my first concern is because um , I 'm interested in what happens for people , random people out in some random place where they 're p having an impromptu discussion .
The other thing actually , that gets at this a little bit of something else I 'd like to do , is what happens if you have two P D
and they communicate with each other ? And then {disfmarker} You know , they 're in random positions , the likelihood that {disfmarker} I mean , basically there wouldn't be any {disfmarker} l likely to be any kind of nulls , if you even had two . If you had three or four it 's {disfmarker} Yeah .
I mean , not only can you do microphone arrays , but you can do all sorts of um multi - band as well .
Um and then also anonymity , how we want to anonymize the data .
the question becomes what symbol are you gonna put in there for everybody 's name ,
and whether you 're gonna put it in the text where he says " Hey Roger " or are we gonna put that person 's anonymized name in instead ?
OK , well , but then there 's this issue of if we 're gonna use this for a discourse type of thing , then {disfmarker} and , you know , Liz was mentioning stuff in a previous meeting about gaze direction and who 's {disfmarker} who 's the addressee and all , then to have " Roger " be the thing in the utterance and then actually have the speaker identifier who was " Roger " be " Frank " , that 's going to be really confusing and make it pretty much useless for discourse analysis .
Um , how important is it for a person to be identified by first name versus full name ?
On the other hand , this is a small {disfmarker} this is a small pool , and people who say things about topic X e who are researchers and well - known in the field , they 'll be identifiable and simply from the {disfmarker} from the first name .
However , taking one step further back , they 'd be identifiable anyway , even if we changed all the names .
Now , it would be very possible for me to take those data put them in a {disfmarker} in a study , and just change everybody 's name for the purpose of the publication .
So again , th the issue is if you 're tracking discourse things , you know , if someone says , uh , uh , " Frank said this " and then you wanna connect it to something later , you 've gotta have this part where that 's " Frank colon " .
Yeah , and {disfmarker} and {disfmarker} you know , even more i i uh , immediate than that just being able to , uh {disfmarker} Well , it just seems like to track {disfmarker} track from one utterance to the next utterance who 's speaking and who 's speaking to whom , cuz that can be important .
Well , I would sug I {disfmarker} I {disfmarker} don't wanna change the names in the transcript ,
but that 's because I 'm focused so much on the acoustics instead of on the discourse , and so I think that 's a really good point .
You 're right , this is going to require more thought .
How will we {disfmarker} how would the person who 's doing the transcript even know who they 're talking about ?
I mean , so so {disfmarker} how is that information gonna get labeled anyway ?
Well , the current one they don't do speaker identity .
because in NaturallySpeaking , or , excuse me , in ViaVoice , it 's only one person .
and so in their current conventions there are no multiple speaker conventions .
And within that , it may be that it 's sufficient to not uh change the {disfmarker} to not incorporate anonymization yet , but always , always in the publications we have to .
I {disfmarker} I think that we {disfmarker} we have a {disfmarker} need to have a consistent licensing policy of some sort , and {disfmarker}
And th and the other thing is if {disfmarker} if {disfmarker} if Liz were here , {vocalsound} what she might say is that she wants to look if things that cut across between the audio and the dialogue ,
Well , you see ? So , it 's complicated .
I think we have to think about w @ @ {comment} how . I think that this can't be decided today .
Because that would give you a mapping between the speaker 's real name and the tag we 're using , and we don't want {disfmarker}
Then , uh , it seems to me that {disfmarker} Well , maybe I {disfmarker} uh it seems to me that if you change the name , the transcript 's gonna disagree with the audio , and you won't be able to use that .
Because if we made the {disfmarker} the transcript be the tag that we 're using for Roger , someone who had the transcript and the audio would then have a mapping between the anonymized name and the real name , and we wanna avoid that .
