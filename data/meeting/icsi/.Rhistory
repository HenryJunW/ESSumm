name = names_test_set[1]
i = 1
for (name in names_test_set){
# read golden (abstractive)
my_path = paste0(my_path_root,"/data/icsi")
candidate_names = list.files(my_path, pattern="\\.longabstract")
names_golden = candidate_names[grepl(name, candidate_names)]
# there are four golden summaries per meeting
# we retain the three summaries written by "beata", "vkaraisk", and "s9553330" for consistency with previous papers
names_golden = names_golden[unlist(lapply(names_golden, function(x) length(unlist(strsplit(x, split="\\.")))))>3]
k = 1
for (name_golden in names_golden){
my_path = paste0(my_path_root,"/data/icsi")
golden_temp = readLines(paste0(my_path,"/",name_golden))
# remove punctuation
golden_temp = lapply(golden_temp, cleaning_for_baselines)
my_path = paste0(my_path_root,"/data/output/icsi/summaries/golden_test")
writeLines(paste0(golden_temp,"."),paste0(my_path,"/",name,"_golden_",k,".txt"))
k = k+1
}
# read extractive summary for the meeting
my_path = paste0(my_path_root,"/data/icsi")
extractive_names_candidates = list.files(my_path, pattern="\\.extsumm")
extractive_names = extractive_names_candidates[grepl(name, extractive_names_candidates)]
k = 1
for (ex_name in extractive_names){
my_path = paste0(my_path_root,"/data/icsi")
ex_temp = readLines(paste0(my_path,"/",ex_name))
# remove punctuation
pseudo_oracle_temp = lapply(ex_temp, cleaning_for_baselines)
# for pseudo_oracle_temp, randomly select sentences until max length is reached
raw_text_pseudo_oracle_temp = paste0(paste0(pseudo_oracle_temp,"."), collapse=" ")
set.seed(5132016)
pseudo_oracle_temp = random_baseline(raw_text = raw_text_pseudo_oracle_temp, max_length = max_summary_lengthes[i])$summary
my_path = paste0(my_path_root,"/data/output/icsi/summaries/pseudo_oracle_baseline")
writeLines(pseudo_oracle_temp,paste0(my_path,"/",name,"_pseudo-oracle-baseline_",k,".txt"))
k = k+1
}
print(i)
i = i+1
}
# save raw text for the baselines
my_path = paste0(my_path_root,"/data/output/icsi/text_for_baselines")
lapply(1:length(asr_info_test), function(x){write(paste0(as.character(asr_info_test[[x]][,"text"]),collapse=". "),file=paste0(my_path,"/",unlist(strsplit(names_test_set[[x]], split="\\."))[1],"_full_raw_text.txt"))})
# raw text from these meetings
my_path = paste0(my_path_root,"/data/output/icsi")
load(paste0(my_path,"/human_info_test"))
########## golden and pseudo oracle baselines
# they stay the same
# save raw text for the baselines
my_path = paste0(my_path_root,"/data/output/icsi/text_for_baselines")
lapply(1:length(human_info_test), function(x){write(paste0(as.character(human_info_test[[x]][,9]),collapse=". "),file=paste0(my_path,"/",unlist(strsplit(names_test_set[[x]], split="\\."))[1],"_full_raw_text_human.txt"))})
head(human_info_test[[1]])
# save raw text for the baselines
my_path = paste0(my_path_root,"/data/output/icsi/text_for_baselines")
lapply(1:length(human_info_test), function(x){write(paste0(as.character(human_info_test[[x]][,"text"]),collapse=". "),file=paste0(my_path,"/",unlist(strsplit(names_test_set[[x]], split="\\."))[1],"_full_raw_text_human.txt"))})
cleaning_for_baselines
head(human_info_test[[1]])
human_info_test[[1]][,1:30]
human_info_test[[1]][1:30,]
my_path = paste0(my_path_root,"/data/output/icsi")
load(paste0(my_path,"/human_info_test"))
human_info_test[[1]][1:30,]
my_path = paste0(my_path_root,"/data/output/icsi/cleaned_transcripts_test")
cleaned_transcripts_test_processed_human_names = list.files(path = my_path, pattern = "_processed_human.csv$")
cleaned_transcripts_test_unprocessed_human_names = list.files(path = my_path, pattern = "unprocessed_human.csv$")
cleaned_transcripts_test_human = list()
for (i in 1:length(cleaned_transcripts_test_processed_human_names)){
cleaned_transcripts_test_human[[i]] = list(unprocessed = read.csv(paste0(my_path,"/", cleaned_transcripts_test_unprocessed_human_names[i])), processed = read.csv(paste0(my_path,"/", cleaned_transcripts_test_processed_human_names[i])))
}
terms_lists_test_human = lapply(cleaned_transcripts_test_human, function(x){list(processed = from_cleaned_transcript_to_terms_list(x$processed[,4])$terms_list_partial, unprocessed = from_cleaned_transcript_to_terms_list(x$unprocessed[,4])$terms_list_partial)})
my_path = paste0(my_path_root,"/data/output/icsi/text_for_baselines")
name
# names of the meetings from the test set
my_path = paste0(my_path_root,"/data/lists")
names_test_set = as.character(read.csv(file = paste0(my_path,"/list.ami.test"), header = FALSE)[,1])
# raw text from these meetings
my_path = paste0(my_path_root,"/data/output/ami")
load(paste0(my_path,"/asr_info_test"))
# save meeting sizes in terms of number of words
test_set_meeting_n_words = unlist(lapply(asr_info_test, function(x){length(unlist(strsplit(paste(as.character(x[,4]),collapse=" "),split=" ")))}))
# compute summary lengthes according to the linear model
my_path = paste0(my_path_root,"/data/output/ami")
load(file=paste0(my_path,"/summary_length_prediction_model"))
max_summary_lengthes = round(as.numeric(predict.lm(fit_2, data.frame(all_number_of_words=test_set_meeting_n_words), type="response")))
my_path = paste0(my_path_root,"/data/lists")
names_test_set = as.character(read.csv(file = paste0(my_path,"/list.ami.test"), header = FALSE)[,1])
name=names_test_set[1]
# read
my_path = paste0(my_path_root,"/data/ami")
golden_temp = readLines(paste0(my_path,"/",name,".longabstract"))
pseudo_oracle_temp = readLines(paste0(my_path,"/",name,".extsumm"))
pseudo_oracle_temp
name
lapply(pseudo_oracle_temp, cleaning_for_baselines)
length(asr_info_test)
asr_info_test[[1]]
asr_info_test[[x]][,"text"]
asr_info_test[[1]][,"text"]
cleaning_for_baselines(asr_info_test[[x]][,"text"])
cleaning_for_baselines(paste0(as.character(asr_info_test[[1]][,"text"]),collapse=". "))
asr_info_test[[1]][,"text"]
paste0(as.character(lapply(asr_info_test[[1]][,"text"], cleaning_for_baselines),collapse=". "))
lapply(asr_info_test[[1]][,"text"], cleaning_for_baselines)
paste0(unlist(lapply(asr_info_test[[1]][,"text"], cleaning_for_baselines)), collapse=". ")
my_path = paste0(my_path_root,"/data/output/ami/text_for_baselines")
lapply(1:length(asr_info_test), function(x){write(paste0(unlist(lapply(asr_info_test[[x]][,"text"], cleaning_for_baselines)), collapse=". "),file=paste0(my_path,"/",unlist(strsplit(names_test_set[[x]], split="\\."))[1],"_full_raw_text.txt"))})
# load transcripts and save as list
my_path = paste0(my_path_root,"/data/output/ami/cleaned_transcripts_test")
cleaned_transcripts_test_processed_names = list.files(path = my_path, pattern = "_processed.csv$")
cleaned_transcripts_test_unprocessed_names = list.files(path = my_path, pattern = "unprocessed.csv$")
cleaned_transcripts_test = list()
for (i in 1:length(cleaned_transcripts_test_processed_names)){
cleaned_transcripts_test[[i]] = list(unprocessed = read.csv(paste0(my_path,"/", cleaned_transcripts_test_unprocessed_names[i])), processed = read.csv(paste0(my_path,"/", cleaned_transcripts_test_processed_names[i])))
}
terms_lists_test = lapply(cleaned_transcripts_test, function(x){list(processed = from_cleaned_transcript_to_terms_list(x$processed[,4])$terms_list_partial, unprocessed = from_cleaned_transcript_to_terms_list(x$unprocessed[,4])$terms_list_partial)})
my_path = paste0(my_path_root,"/data/output/ami/text_for_baselines")
i = 1
for (name in names_test_set){
max_length = max_summary_lengthes[i]
################################### baselines (except TextRank and ClusterRank, which are performed in Python)
# load raw text
raw_text_temp = readLines(paste0(my_path,"/",name,"_full_raw_text.txt"))
random_baseline_summaries[[i]] = random_baseline(raw_text = raw_text_temp, max_length = max_length)$summary
longest_greedy_baseline_summaries[[i]] = longest_greedy_baseline(raw_text = raw_text_temp, max_length = max_length)$summary
################################### our best model
utterances = as.character(cleaned_transcripts_test[[i]]$unprocessed[,4])
start_time = cleaned_transcripts_test[[i]]$unprocessed[,1]
ptm = proc.time()["elapsed"]
# prune out utterances that are too short
utterances_lengthes = unlist(lapply(utterances, function(x){length(setdiff(unlist(strsplit(tolower(x),split=" ")), custom_stopwords))}))
index_remove = which(utterances_lengthes<=3)
if (length(index_remove)>0){
utterances = utterances[-index_remove]
start_time = start_time[-index_remove]
}
# clean utterances
utterances = clean_utterances(utterances, my_stopwords=custom_stopwords)$utterances
terms_list = terms_lists_test[[i]]
model_summaries[[i]] = from_terms_to_summary(terms_list, utterances, start_time, window_size, to_overspan, to_build_on_processed, community_algo, weighted_comm, directed_comm, rw_length, size_threshold, degeneracy, directed_mode, method, use_elbow, use_percentage, percentage, number_to_retain, which_nodes, redundancy_threshold, to_stem, filler_words, my_stopwords=custom_stopwords, max_summary_length=max_length)$my_summary
processing_times[[i]] = round(proc.time()["elapsed"]  - ptm, 2)
################ best model for long documents from the keyword_extraction paper - out of curiosity
crp_summaries[[i]] = from_terms_to_summary(terms_list, utterances, start_time, window_size=20, to_overspan=T, to_build_on_processed=T, community_algo="none", weighted_comm=NA, directed_comm=NA, rw_length=NA, size_threshold=NA, degeneracy="weighted_k_core", directed_mode="all", method="CRP", use_elbow=NA, use_percentage=NA, percentage=0.1, number_to_retain=NA, which_nodes="all", redundancy_threshold, to_stem, filler_words, my_stopwords=custom_stopwords, max_length)$my_summary
print (i)
i = i+1
}
# for the summaries generated by our model(s), collapse intra-word apostrophes to meet ROUGE requirements
model_summaries=lapply(model_summaries, function(x)  {
# remove non intra-word apostrophes
x = gsub("' ", " ", x, perl = TRUE)
x = gsub(" '", " ", x, perl = TRUE)
# collapse intra-word apostrophes
x = gsub("'", "", x, perl = TRUE)
return(x)
})
# for the summaries generated by our model(s), collapse intra-word apostrophes to meet ROUGE requirements
crp_summaries=lapply(crp_summaries, function(x)  {
# remove non intra-word apostrophes
x = gsub("' ", " ", x, perl = TRUE)
x = gsub(" '", " ", x, perl = TRUE)
# collapse intra-word apostrophes
x = gsub("'", "", x, perl = TRUE)
return(x)
})
# write summaries to files
my_path = paste0(my_path_root,"/data/output/ami/summaries/random_baseline")
lapply(1:length(names_test_set), function(x){writeLines(random_baseline_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_random-baseline.txt"))})
my_path = paste0(my_path_root,"/data/output/ami/summaries/longest_greedy_baseline")
lapply(1:length(names_test_set), function(x){writeLines(longest_greedy_baseline_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_longest-greedy-baseline.txt"))})
my_path = paste0(my_path_root,"/data/output/ami/summaries/best_model")
lapply(1:length(names_test_set), function(x){writeLines(model_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_best-model.txt"))})
my_path = paste0(my_path_root,"/data/output/ami/summaries/crp")
lapply(1:length(names_test_set), function(x){writeLines(crp_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_crp-model.txt"))})
my_path = paste0(my_path_root,"/data/output/ami")
load(paste0(my_path,"/human_info_test"))
########## golden and pseudo oracle baselines
# they both stay the same
# save raw text for the baselines
my_path = paste0(my_path_root,"/data/output/ami/text_for_baselines")
lapply(1:length(asr_info_test), function(x){write(paste0(unlist(lapply(human_info_test[[x]][,"text"], cleaning_for_baselines)), collapse=". "),file=paste0(my_path,"/",unlist(strsplit(names_test_set[[x]], split="\\."))[1],"_full_raw_text_human.txt"))})
# new up to here above
##########
# load transcripts and save as list
my_path = paste0(my_path_root,"/data/output/ami/cleaned_transcripts_test")
cleaned_transcripts_test_processed_human_names = list.files(path = my_path, pattern = "_processed_human.csv$")
cleaned_transcripts_test_unprocessed_human_names = list.files(path = my_path, pattern = "unprocessed_human.csv$")
cleaned_transcripts_test_human = list()
for (i in 1:length(cleaned_transcripts_test_processed_human_names)){
cleaned_transcripts_test_human[[i]] = list(unprocessed = read.csv(paste0(my_path,"/", cleaned_transcripts_test_unprocessed_human_names[i])), processed = read.csv(paste0(my_path,"/", cleaned_transcripts_test_processed_human_names[i])))
}
terms_lists_test_human = lapply(cleaned_transcripts_test_human, function(x){list(processed = from_cleaned_transcript_to_terms_list(x$processed[,4])$terms_list_partial, unprocessed = from_cleaned_transcript_to_terms_list(x$unprocessed[,4])$terms_list_partial)})
my_path = paste0(my_path_root,"/data/output/ami/text_for_baselines")
#### best parameter values (our best model)
model_summaries = list()
crp_summaries = list()
random_baseline_summaries = list()
longest_greedy_baseline_summaries = list()
processing_times = list()
my_path = paste0(my_path_root,"/data/output/ami/text_for_baselines")
i = 1
for (name in names_test_set){
max_length = max_summary_lengthes[i]
################################### baselines (except TextRank and ClusterRank, which are performed in Python)
# load raw text
raw_text_temp = readLines(paste0(my_path,"/",name,"_full_raw_text_human.txt"))
random_baseline_summaries[[i]] = random_baseline(raw_text = raw_text_temp, max_length = max_length)$summary
longest_greedy_baseline_summaries[[i]] = longest_greedy_baseline(raw_text = raw_text_temp, max_length = max_length)$summary
################################### our best model
utterances = as.character(cleaned_transcripts_test_human[[i]]$unprocessed[,4])
start_time = cleaned_transcripts_test_human[[i]]$unprocessed[,1]
ptm = proc.time()["elapsed"]
# prune out utterances that are too short
utterances_lengthes = unlist(lapply(utterances, function(x){length(setdiff(unlist(strsplit(tolower(x),split=" ")), custom_stopwords))}))
index_remove = which(utterances_lengthes<=3)
if (length(index_remove)>0){
utterances = utterances[-index_remove]
start_time = start_time[-index_remove]
}
# clean utterances
utterances = clean_utterances(utterances, my_stopwords=custom_stopwords)$utterances
terms_list = terms_lists_test_human[[i]]
model_summaries[[i]] = from_terms_to_summary(terms_list, utterances, start_time, window_size, to_overspan, to_build_on_processed, community_algo, weighted_comm, directed_comm, rw_length, size_threshold, degeneracy, directed_mode, method, use_elbow, use_percentage, percentage, number_to_retain, which_nodes, redundancy_threshold, to_stem, filler_words, my_stopwords=custom_stopwords, max_summary_length=max_length)$my_summary
processing_times[[i]] = round(proc.time()["elapsed"]  - ptm, 2)
################ best model for long documents from the keyword_extraction paper - out of curiosity
crp_summaries[[i]] = from_terms_to_summary(terms_list, utterances, start_time, window_size=20, to_overspan=T, to_build_on_processed=T, community_algo="none", weighted_comm=NA, directed_comm=NA, rw_length=NA, size_threshold=NA, degeneracy="weighted_k_core", directed_mode="all", method="CRP", use_elbow=NA, use_percentage=NA, percentage=0.1, number_to_retain=NA, which_nodes="all", redundancy_threshold, to_stem, filler_words, my_stopwords=custom_stopwords, max_length)$my_summary
print (i)
i = i+1
}
# for the summaries generated by our model(s), collapse intra-word apostrophes to meet ROUGE requirements
model_summaries = lapply(model_summaries, function(x)  {
# remove non intra-word apostrophes
x = gsub("' ", " ", x, perl = TRUE)
x = gsub(" '", " ", x, perl = TRUE)
# collapse intra-word apostrophes
x = gsub("'", "", x, perl = TRUE)
return(x)
})
# for the summaries generated by our model(s), collapse intra-word apostrophes to meet ROUGE requirements
crp_summaries = lapply(crp_summaries, function(x)  {
# remove non intra-word apostrophes
x = gsub("' ", " ", x, perl = TRUE)
x = gsub(" '", " ", x, perl = TRUE)
# collapse intra-word apostrophes
x = gsub("'", "", x, perl = TRUE)
return(x)
})
# write summaries to files
my_path = paste0(my_path_root,"/data/output/ami/summaries/random_baseline")
lapply(1:length(names_test_set), function(x){writeLines(random_baseline_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_random-baseline-human.txt"))})
my_path = paste0(my_path_root,"/data/output/ami/summaries/longest_greedy_baseline")
lapply(1:length(names_test_set), function(x){writeLines(longest_greedy_baseline_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_longest-greedy-baseline-human.txt"))})
my_path = paste0(my_path_root,"/data/output/ami/summaries/best_model")
lapply(1:length(names_test_set), function(x){writeLines(model_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_best-model-human.txt"))})
my_path = paste0(my_path_root,"/data/output/ami/summaries/crp")
lapply(1:length(names_test_set), function(x){writeLines(crp_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_crp-model-human.txt"))})
# names of the meetings from the test set
my_path = paste0(my_path_root,"/data/lists")
names_test_set = as.character(read.csv(file = paste0(my_path,"/list.icsi.test"), header = FALSE)[,1])
# raw text from these meetings
my_path = paste0(my_path_root,"/data/output/icsi")
load(paste0(my_path,"/asr_info_test"))
# save meeting sizes in terms of number of words
test_set_meeting_n_words = unlist(lapply(asr_info_test, function(x){length(unlist(strsplit(paste(as.character(x[,4]),collapse=" "),split=" ")))}))
# compute summary lengthes according to the linear model
my_path = paste0(my_path_root,"/data/output/ami")
load(file=paste0(my_path,"/summary_length_prediction_model"))
max_summary_lengthes = round(as.numeric(predict.lm(fit_2, data.frame(all_number_of_words=test_set_meeting_n_words), type="response")))
# names of the meetings from the test set
my_path = paste0(my_path_root,"/data/lists")
names_test_set = as.character(read.csv(file = paste0(my_path,"/list.icsi.test"), header = FALSE)[,1])
# raw text from these meetings
my_path = paste0(my_path_root,"/data/output/icsi")
load(paste0(my_path,"/asr_info_test"))
# save meeting sizes in terms of number of words
test_set_meeting_n_words = unlist(lapply(asr_info_test, function(x){length(unlist(strsplit(paste(as.character(x[,4]),collapse=" "),split=" ")))}))
# compute summary lengthes according to the linear model
my_path = paste0(my_path_root,"/data/output/ami")
load(file=paste0(my_path,"/summary_length_prediction_model"))
max_summary_lengthes = round(as.numeric(predict.lm(fit_2, data.frame(all_number_of_words=test_set_meeting_n_words), type="response")))
i = 1
for (name in names_test_set){
# read golden (abstractive)
my_path = paste0(my_path_root,"/data/icsi")
candidate_names = list.files(my_path, pattern="\\.longabstract")
names_golden = candidate_names[grepl(name, candidate_names)]
# there are four golden summaries per meeting
# we retain the three summaries written by "beata", "vkaraisk", and "s9553330" for consistency with previous papers
names_golden = names_golden[unlist(lapply(names_golden, function(x) length(unlist(strsplit(x, split="\\.")))))>3]
k = 1
for (name_golden in names_golden){
my_path = paste0(my_path_root,"/data/icsi")
golden_temp = readLines(paste0(my_path,"/",name_golden))
# remove punctuation
golden_temp = lapply(golden_temp, cleaning_for_baselines)
my_path = paste0(my_path_root,"/data/output/icsi/summaries/golden_test")
writeLines(paste0(golden_temp,"."),paste0(my_path,"/",name,"_golden_",k,".txt"))
k = k+1
}
# read extractive summary for the meeting
my_path = paste0(my_path_root,"/data/icsi")
extractive_names_candidates = list.files(my_path, pattern="\\.extsumm")
extractive_names = extractive_names_candidates[grepl(name, extractive_names_candidates)]
k = 1
for (ex_name in extractive_names){
my_path = paste0(my_path_root,"/data/icsi")
ex_temp = readLines(paste0(my_path,"/",ex_name))
# remove punctuation
pseudo_oracle_temp = lapply(ex_temp, cleaning_for_baselines)
# for pseudo_oracle_temp, randomly select sentences until max length is reached
raw_text_pseudo_oracle_temp = paste0(paste0(pseudo_oracle_temp,"."), collapse=" ")
set.seed(5132016)
pseudo_oracle_temp = random_baseline(raw_text = raw_text_pseudo_oracle_temp, max_length = max_summary_lengthes[i])$summary
my_path = paste0(my_path_root,"/data/output/icsi/summaries/pseudo_oracle_baseline")
writeLines(pseudo_oracle_temp,paste0(my_path,"/",name,"_pseudo-oracle-baseline_",k,".txt"))
k = k+1
}
print(i)
i = i+1
}
my_path = paste0(my_path_root,"/data/output/icsi/text_for_baselines")
lapply(1:length(asr_info_test), function(x){write(paste0(unlist(lapply(asr_info_test[[x]][,"text"], cleaning_for_baselines)), collapse=". "),file=paste0(my_path,"/",unlist(strsplit(names_test_set[[x]], split="\\."))[1],"_full_raw_text.txt"))})
# load transcripts and save as list
my_path = paste0(my_path_root,"/data/output/icsi/cleaned_transcripts_test")
cleaned_transcripts_test_processed_names = list.files(path = my_path, pattern = "_processed.csv$")
cleaned_transcripts_test_unprocessed_names = list.files(path = my_path, pattern = "unprocessed.csv$")
cleaned_transcripts_test = list()
for (i in 1:length(cleaned_transcripts_test_processed_names)){
cleaned_transcripts_test[[i]] = list(unprocessed = read.csv(paste0(my_path,"/", cleaned_transcripts_test_unprocessed_names[i])), processed = read.csv(paste0(my_path,"/", cleaned_transcripts_test_processed_names[i])))
}
terms_lists_test = lapply(cleaned_transcripts_test, function(x){list(processed = from_cleaned_transcript_to_terms_list(x$processed[,4])$terms_list_partial, unprocessed = from_cleaned_transcript_to_terms_list(x$unprocessed[,4])$terms_list_partial)})
terms_lists_test[[1]]
my_path = paste0(my_path_root,"/data/output/icsi/text_for_baselines")
cleaned_transcripts_test[[1]]$unprocessed[,4]
filler_words
# save raw text for the baselines
my_path = paste0(my_path_root,"/data/output/icsi/text_for_baselines")
lapply(1:length(asr_info_test), function(x){write(paste0(unlist(lapply(asr_info_test[[x]][,"text"], cleaning_for_baselines)), collapse=". "),file=paste0(my_path,"/",unlist(strsplit(names_test_set[[x]], split="\\."))[1],"_full_raw_text.txt"))})
# load transcripts and save as list
my_path = paste0(my_path_root,"/data/output/icsi/cleaned_transcripts_test")
cleaned_transcripts_test_processed_names = list.files(path = my_path, pattern = "_processed.csv$")
cleaned_transcripts_test_unprocessed_names = list.files(path = my_path, pattern = "unprocessed.csv$")
cleaned_transcripts_test = list()
for (i in 1:length(cleaned_transcripts_test_processed_names)){
cleaned_transcripts_test[[i]] = list(unprocessed = read.csv(paste0(my_path,"/", cleaned_transcripts_test_unprocessed_names[i])), processed = read.csv(paste0(my_path,"/", cleaned_transcripts_test_processed_names[i])))
}
terms_lists_test = lapply(cleaned_transcripts_test, function(x){list(processed = from_cleaned_transcript_to_terms_list(x$processed[,4])$terms_list_partial, unprocessed = from_cleaned_transcript_to_terms_list(x$unprocessed[,4])$terms_list_partial)})
my_path = paste0(my_path_root,"/data/output/icsi/text_for_baselines")
cleaned_transcripts_test[[2]]$unprocessed[,4]
names_test_set[2]
cleaned_transcripts_test[[2]]$unprocessed[,4][1:10]
model_summaries = list()
crp_summaries = list()
random_baseline_summaries= list()
longest_greedy_baseline_summaries = list()
processing_times = list()
my_path = paste0(my_path_root,"/data/output/icsi/text_for_baselines")
i = 1
for (name in names_test_set){
max_length = max_summary_lengthes[i]
################################### baselines (except TextRank and ClusterRank, which are performed in Python)
# load raw text
raw_text_temp = readLines(paste0(my_path,"/",name,"_full_raw_text.txt"))
random_baseline_summaries[[i]] = random_baseline(raw_text = raw_text_temp, max_length = max_length)$summary
longest_greedy_baseline_summaries[[i]] = longest_greedy_baseline(raw_text = raw_text_temp, max_length = max_length)$summary
################################### our best model
utterances = as.character(cleaned_transcripts_test[[i]]$unprocessed[,4])
start_time = cleaned_transcripts_test[[i]]$unprocessed[,1]
ptm = proc.time()["elapsed"]
# prune out utterances that are too short
utterances_lengthes = unlist(lapply(utterances, function(x){length(setdiff(unlist(strsplit(tolower(x),split=" ")), custom_stopwords))}))
index_remove = which(utterances_lengthes<=3)
if (length(index_remove)>0){
utterances = utterances[-index_remove]
start_time = start_time[-index_remove]
}
# clean utterances
utterances = clean_utterances(utterances, my_stopwords=custom_stopwords)$utterances
terms_list = terms_lists_test[[i]]
model_summaries[[i]] = from_terms_to_summary(terms_list, utterances, start_time, window_size, to_overspan, to_build_on_processed, community_algo, weighted_comm, directed_comm, rw_length, size_threshold, degeneracy, directed_mode, method, use_elbow, use_percentage, percentage, number_to_retain, which_nodes, redundancy_threshold, to_stem, filler_words, my_stopwords=custom_stopwords, max_summary_length=max_length)$my_summary
processing_times[[i]] = round(proc.time()["elapsed"]  - ptm, 2)
################ best model for long documents from the keyword_extraction paper - out of curiosity
crp_summaries[[i]] = from_terms_to_summary(terms_list, utterances, start_time, window_size=20, to_overspan=T, to_build_on_processed=T, community_algo="none", weighted_comm=NA, directed_comm=NA, rw_length=NA, size_threshold=NA, degeneracy="weighted_k_core", directed_mode="all", method="CRP", use_elbow=NA, use_percentage=NA, percentage=0.1, number_to_retain=NA, which_nodes="all", redundancy_threshold, to_stem, filler_words, my_stopwords=custom_stopwords, max_length)$my_summary
print (i)
i = i+1
}
# for the summaries generated by our model(s), collapse intra-word apostrophes to meet ROUGE requirements
model_summaries=lapply(model_summaries, function(x)  {
# remove non intra-word apostrophes
x = gsub("' ", " ", x, perl = TRUE)
x = gsub(" '", " ", x, perl = TRUE)
# collapse intra-word apostrophes
x = gsub("'", "", x, perl = TRUE)
return(x)
})
# for the summaries generated by our model(s), collapse intra-word apostrophes to meet ROUGE requirements
crp_summaries=lapply(crp_summaries, function(x)  {
# remove non intra-word apostrophes
x = gsub("' ", " ", x, perl = TRUE)
x = gsub(" '", " ", x, perl = TRUE)
# collapse intra-word apostrophes
x = gsub("'", "", x, perl = TRUE)
return(x)
})
# for the summaries generated by our model(s), collapse intra-word apostrophes to meet ROUGE requirements
model_summaries=lapply(model_summaries, function(x)  {
# remove non intra-word apostrophes
x = gsub("' ", " ", x, perl = TRUE)
x = gsub(" '", " ", x, perl = TRUE)
# collapse intra-word apostrophes
x = gsub("'", "", x, perl = TRUE)
return(x)
})
# for the summaries generated by our model(s), collapse intra-word apostrophes to meet ROUGE requirements
crp_summaries=lapply(crp_summaries, function(x)  {
# remove non intra-word apostrophes
x = gsub("' ", " ", x, perl = TRUE)
x = gsub(" '", " ", x, perl = TRUE)
# collapse intra-word apostrophes
x = gsub("'", "", x, perl = TRUE)
return(x)
})
# write summaries to files
my_path = paste0(my_path_root,"/data/output/icsi/summaries/random_baseline")
lapply(1:length(names_test_set), function(x){writeLines(random_baseline_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_random-baseline.txt"))})
my_path = paste0(my_path_root,"/data/output/icsi/summaries/longest_greedy_baseline")
lapply(1:length(names_test_set), function(x){writeLines(longest_greedy_baseline_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_longest-greedy-baseline.txt"))})
my_path = paste0(my_path_root,"/data/output/icsi/summaries/best_model")
lapply(1:length(names_test_set), function(x){writeLines(model_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_best-model.txt"))})
my_path = paste0(my_path_root,"/data/output/icsi/summaries/crp")
lapply(1:length(names_test_set), function(x){writeLines(crp_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_crp-model.txt"))})
# raw text from these meetings
my_path = paste0(my_path_root,"/data/output/icsi")
load(paste0(my_path,"/human_info_test"))
########## golden and pseudo oracle baselines
# they stay the same NOT MODIFY THAT #TODO
# save raw text for the baselines
my_path = paste0(my_path_root,"/data/output/icsi/text_for_baselines")
lapply(1:length(human_info_test), function(x){write(paste0(unlist(lapply(human_info_test[[x]][,"text"], cleaning_for_baselines)), collapse=". "),file=paste0(my_path,"/",unlist(strsplit(names_test_set[[x]], split="\\."))[1],"_full_raw_text_human.txt"))})
# load transcripts and save as list
my_path = paste0(my_path_root,"/data/output/icsi/cleaned_transcripts_test")
cleaned_transcripts_test_processed_human_names = list.files(path = my_path, pattern = "_processed_human.csv$")
cleaned_transcripts_test_unprocessed_human_names = list.files(path = my_path, pattern = "unprocessed_human.csv$")
cleaned_transcripts_test_human = list()
for (i in 1:length(cleaned_transcripts_test_processed_human_names)){
cleaned_transcripts_test_human[[i]] = list(unprocessed = read.csv(paste0(my_path,"/", cleaned_transcripts_test_unprocessed_human_names[i])), processed = read.csv(paste0(my_path,"/", cleaned_transcripts_test_processed_human_names[i])))
}
terms_lists_test_human = lapply(cleaned_transcripts_test_human, function(x){list(processed = from_cleaned_transcript_to_terms_list(x$processed[,4])$terms_list_partial, unprocessed = from_cleaned_transcript_to_terms_list(x$unprocessed[,4])$terms_list_partial)})
my_path = paste0(my_path_root,"/data/output/icsi/text_for_baselines")
#### best parameter values (our best model)
model_summaries = list()
crp_summaries = list()
random_baseline_summaries = list()
longest_greedy_baseline_summaries = list()
processing_times = list()
my_path = paste0(my_path_root,"/data/output/icsi/text_for_baselines")
i = 1
for (name in names_test_set){
max_length = max_summary_lengthes[i]
################################### baselines (except TextRank and ClusterRank, which are performed in Python)
# load raw text
raw_text_temp = readLines(paste0(my_path,"/",name,"_full_raw_text_human.txt"))
random_baseline_summaries[[i]] = random_baseline(raw_text = raw_text_temp, max_length = max_length)$summary
longest_greedy_baseline_summaries[[i]] = longest_greedy_baseline(raw_text = raw_text_temp, max_length = max_length)$summary
################################### our best model
utterances = as.character(cleaned_transcripts_test_human[[i]]$unprocessed[,4])
start_time = cleaned_transcripts_test_human[[i]]$unprocessed[,1]
ptm = proc.time()["elapsed"]
# prune out utterances that are too short
utterances_lengthes = unlist(lapply(utterances, function(x){length(setdiff(unlist(strsplit(tolower(x),split=" ")), custom_stopwords))}))
index_remove = which(utterances_lengthes<=3)
if (length(index_remove)>0){
utterances = utterances[-index_remove]
start_time = start_time[-index_remove]
}
# clean utterances
utterances = clean_utterances(utterances, my_stopwords=custom_stopwords)$utterances
terms_list = terms_lists_test_human[[i]]
model_summaries[[i]] = from_terms_to_summary(terms_list, utterances, start_time, window_size, to_overspan, to_build_on_processed, community_algo, weighted_comm, directed_comm, rw_length, size_threshold, degeneracy, directed_mode, method, use_elbow, use_percentage, percentage, number_to_retain, which_nodes, redundancy_threshold, to_stem, filler_words, my_stopwords=custom_stopwords, max_summary_length=max_length)$my_summary
processing_times[[i]] = round(proc.time()["elapsed"]  - ptm, 2)
################ best model for long documents from the keyword_extraction paper - out of curiosity
crp_summaries[[i]] = from_terms_to_summary(terms_list, utterances, start_time, window_size=20, to_overspan=T, to_build_on_processed=T, community_algo="none", weighted_comm=NA, directed_comm=NA, rw_length=NA, size_threshold=NA, degeneracy="weighted_k_core", directed_mode="all", method="CRP", use_elbow=NA, use_percentage=NA, percentage=0.1, number_to_retain=NA, which_nodes="all", redundancy_threshold, to_stem, filler_words, my_stopwords=custom_stopwords, max_length)$my_summary
print (i)
i = i+1
}
# for the summaries generated by our model(s), collapse intra-word apostrophes to meet ROUGE requirements
model_summaries = lapply(model_summaries, function(x)  {
# remove non intra-word apostrophes
x = gsub("' ", " ", x, perl = TRUE)
x = gsub(" '", " ", x, perl = TRUE)
# collapse intra-word apostrophes
x = gsub("'", "", x, perl = TRUE)
return(x)
})
# for the summaries generated by our model(s), collapse intra-word apostrophes to meet ROUGE requirements
crp_summaries = lapply(crp_summaries, function(x)  {
# remove non intra-word apostrophes
x = gsub("' ", " ", x, perl = TRUE)
x = gsub(" '", " ", x, perl = TRUE)
# collapse intra-word apostrophes
x = gsub("'", "", x, perl = TRUE)
return(x)
})
# write summaries to files
my_path = paste0(my_path_root,"/data/output/icsi/summaries/random_baseline")
lapply(1:length(names_test_set), function(x){writeLines(random_baseline_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_random-baseline-human.txt"))})
my_path = paste0(my_path_root,"/data/output/icsi/summaries/longest_greedy_baseline")
lapply(1:length(names_test_set), function(x){writeLines(longest_greedy_baseline_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_longest-greedy-baseline-human.txt"))})
name="Bed016"
my_path = paste0(my_path_root,"/data/output/icsi/summaries/best_model")
lapply(1:length(names_test_set), function(x){writeLines(model_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_best-model-human.txt"))})
my_path = paste0(my_path_root,"/data/output/icsi/summaries/crp")
lapply(1:length(names_test_set), function(x){writeLines(crp_summaries[[x]],paste0(my_path,"/",names_test_set[x],"_crp-model-human.txt"))})
