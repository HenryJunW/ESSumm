So the one issue was that the {disfmarker} the , uh , lapel mike , uh , isn't as good as you would like .
And so , uh , it {disfmarker} it 'd be better if we had close talking mikes for everybody .
And actually in addition to that , that the {disfmarker} the close talking mikes are worn in such a way as to best capture the signal .
it adds this extra , you know , vari variable for each speaker to {disfmarker} to deal with when the microphones aren't similar .
uh , well one thing I was gonna say was that , um , i we could get more , uh , of the head mounted microphones
So it 's towards the corner of your mouth so that breath sounds don't get on it .
And then just sort of about , uh , a thumb or {disfmarker} a thumb and a half away from your {disfmarker} from your mouth .
But if we could actually standardize , you know , the {disfmarker} the microphones , uh , as much as possible that would be really helpful .
so why don't we just go out and {disfmarker} and get an order of {disfmarker} of
uh , I 'd just get a half dozen of these things .
So , uh {disfmarker} so we should go out to our full complement of whatever we can do ,
but have them all be the same mike .
I think the original reason that it was done the other way was because , it w it was sort of an experimental thing
and I don't think anybody knew whether people would rather have more variety or {disfmarker} {vocalsound} or , uh , more uniformity ,
It 's the equipment and also how it 's worn .
it 's really {disfmarker} {nonvocalsound} it makes a big difference from the transcribers ' point of view
Yeah , I think that the point of doing the close talking mike is to get a good quality signal . We 're not doing research on close talking mikes .
So we might as well get it as uniform as we can .
So , as {disfmarker} as I said , we 'll do a field trip and see if we can get all of the same mike that 's more comfortable than {disfmarker} than these things , which I think are horrible .
And , um . OK , in terms of the multi - trans , well that {disfmarker} that 's being modified by Dave Gelbart to , uh , handle multi - channel recording .
And , uh , that 's {disfmarker} that will enable us to do {pause} nice um , tight time marking of the beginning and ending of overlapping segments .
At present it 's not possible with limitations of {disfmarker} of the , uh , original {pause} design of the software .
In terms of , like , pre - segmentation , that {disfmarker} that continues to be , um , a terrific asset to the {disfmarker} to the transcribers .
What {disfmarker} what I 'm doing right now is I 'm trying to include some information about which channel , uh , there 's some speech in .
I 'm just trying to do this by comparing energies , uh {disfmarker} normalizing energies and comparing energies of the different channels .
so to {disfmarker} to give the transcribers some information in which channel there 's {disfmarker} there 's speech in addition to {disfmarker} to the thing we {disfmarker} we did
now which is just , uh , speech - nonspeech detection on the mixed file .
So I 'm {disfmarker} I 'm relying on {disfmarker} on the segmentation of the mixed file
but I 'm {disfmarker} I 'm trying to subdivide the speech portions into different portions if there is some activity in {disfmarker} in different channels .
You know , maybe they 'd wanna stick an array mike here when we 're doing things
uh , Dave Gelbart and I will be , uh , visiting with John Canny
who i you know , is a CS professor ,
who 's interested in ar in array microphones .
but they might wanna just , {disfmarker} uh , you know , you could imagine them taking the four signals from these {disfmarker} these table mikes and trying to do something with them {disfmarker}
That {disfmarker} that reminds me , I had a {disfmarker} a thought of an interesting project that somebody could try to do with {pause} the data from here ,
and that is to try to construct a map of where people were sitting ,
Well Dan {disfmarker} Dan had worked on that . Dan Ellis ,
So that {disfmarker} that 's the cross - correlation stuff , was {disfmarker} was doing b beam - forming .
And so you could plot out who was sitting next to who
if I 'm speaking , or if you 're speaking , or someone over there is speaking , it {disfmarker} if you look at cross - correlation functions , you end up with a {disfmarker}
if {disfmarker} if someone who was on the axis between the two is talking , then you {disfmarker} you get a big peak there .
And then , uh , it {disfmarker} it {disfmarker} it even looks different if th t if the two {disfmarker} two people on either side are talking than if one in the middle .
It {disfmarker} it actually looks somewhat different ,
The reason I didn't go for that here was because , uh , the focus , uh , both of my interest and of Adam 's interest was uh , in impromptu situations .
All {disfmarker} all I know is that they 've been talking to me about a project that they 're going to start up recording people meet in meetings .
NIST has {disfmarker} has done a big meeting room {disfmarker} instrumented meeting room with video and microphone arrays , and very elaborate software .
They wanted to do it {disfmarker}
Uh , well I think they 've instrumented a room
but I don't {pause} think they {disfmarker} they haven't started recordings yet .
And , uh , it is related to ours .
They were interested in ours .
They wanted to get some uniformity with us , uh , about the transcriptions and so on .
but one {disfmarker} one , uh , difference from the audio side was that they are interested in using array mikes .
we 're not recording a bunch of impromptu situations
But the thing we ultimately wanted to aim at was a situation where you were talking with , uh , one or more other people i uh , in {disfmarker} in an p impromptu way , where you didn't {disfmarker} didn't actually know what the situation was going to be .
And therefore it would not {disfmarker} it 'd be highly unlikely that room would be outfitted with {disfmarker} with some very carefully designed array of microphones .
So it 's {disfmarker} it 's a good thing to do , but it doesn't solve the problem of how do you solve things when there 's one mike or at best two mikes in {disfmarker} in this imagined PDA that we have .
So maybe {disfmarker} maybe we 'll do some more of it .
it seems to me that there 's {disfmarker} you know , there are good political reasons for {disfmarker} for doing this ,
You know , it 'd be nice if we can have at least , uh , make use of the data that we 're recording as we go
since it 's sort of {disfmarker} this is the first site that has really collected these really impromptu meetings ,
um , and just have this other information available .
So , if we can get the investment in just for the infra infrastructure
it 'd be g it 'd be good to have {disfmarker} have the recording . I think .
You mean to {disfmarker} to actually get a microphone array and do that ?
Um , but it {disfmarker} definitely in the case of microphone arrays , since if there was a community interested in this , then {disfmarker}
And so I think we could get a microphone array in here pretty easily
and , uh , have it mixed to {disfmarker} to one channel of some sort .
But , e I think for
For {disfmarker} for maximum flexibility later you really don't want to end up with just one channel that 's pointed in the direction of the {disfmarker} the {disfmarker} the p the person with the maximum energy or something like that .
I mean , you {disfmarker} you want actually to {disfmarker} you want actually to have multiple channels being recorded so that you can {disfmarker}
I 'm not so much worried about disk space actually .
but the real issue is that , uh , there is no way to do a recording extended to what we have now with low skew .
but if you 're d i the kind of person who 's doing array processing you actually care about funny little times .
And {disfmarker} and so you actually wou would want to have a completely different set up than we have ,
one that would go up to thirty - two channels or something .
So , I 'm kinda skeptical ,
But what we could do is if there was someone else who 's interested they could have a separate set up which they wouldn't be trying to synch with ours
which might be useful for {disfmarker} for them .
Yeah , we can o offer the meetings , and the physical space ,
and {disfmarker} and {disfmarker} yeah , the transcripts , and so on .
Is there an interest in getting video recordings for these meetings ?
Yes , absolutely . But it 's exactly the same problem ,
you have a problem with people not wanting to be video taped ,
that you have an infrastructure problem ,
and you have the problem that no one who 's currently involved in the project is really hot to do it .
So there 's not enough interest to overcome all of {disfmarker}
but I know there is interest from other places that are interested in looking at meeting data and having the video .
There 's this human subjects problem .
So I think NIST or LDC , or somebody like that I think is much better shape to do all that .
I mean , the thing is , once you actually have serious interest in any of these things then you actually have to put a lot of effort in .
So first of all , um , I 've got eight transcribers .
But the pre - segmentation really helps a huge amount .
And , uh , meetings , you know , I think that they 're {disfmarker} they go as long as a {disfmarker} almost two hours in some {disfmarker} in some cases .
Uh , seven of them are linguists .
One of them is a graduate student in psychology .
And , uh , also Dan Ellis 's innovation of the , uh {disfmarker} the multi - channel to here really helped a r a lot in terms of clearing {disfmarker} clearing up h hearings that involve overlaps .
just out of curiosity I asked one of them how long {pause} it was taking her , one of these two who has already finished her data set .
She said it takes about , uh , sixty minutes transcription for every five minutes of real time .
which is what we were thinking .
It 's well in the range .
but {disfmarker} But it 's word level , speaker change , the things that were mentioned .
And so it 's {disfmarker} it 's a data representation and a set of tools for manipulating transcription graphs of various types .
The {disfmarker} the one {disfmarker} the {disfmarker} what I think you 're referring to , they {disfmarker} they have this concept of an an annotated transcription graph representation .
OK , now I wanted to mention the , um , teleconference I had with , uh , Jonathan Fiscus .
He , um , um , he in indicated to me that they 've {disfmarker} that he 's been ,
but spending a lot of time with the ATLAS system .
But it looks to me like that 's the name that has developed for the system that Bird and Liberman developed {comment} for the annotated {pause} graphs approach .
and what we {disfmarker} what we will do and {disfmarker} uh , is to provide them with the u already transcribed meeting
for him to be able to experiment with in this ATLAS System .
and that he wants to experiment with taking our data
and putting them in that format , and see how that works out .
Well , except I can say that my transcribers use the mixed signal mostly
unless there 's a huge disparity in terms of the volume on {disfmarker} on the mix .
In which case , you know , they {disfmarker} they wouldn't be able to catch anything except the prominent {comment} channel ,
then they 'll switch between .
But I mean we had this {disfmarker} we 've had this discussion many times .
And the answer is we don't actually know the answer because we haven't tried both ways .
I told him he could SSH on and use multi - trans , and have a look at the already done , uh , transcription . And he {disfmarker} and he did .
And what he said was that , um , what they 'll be providing is {disfmarker} will not be as fine grained in terms of the time information .
Um , OK , now I also wanted to say in a different {disfmarker} a different direction is , Brian Kingsbury .
He downloaded {pause} from the CD onto audio tapes .
And apparently he did it one channel per audio tape .
So each of these people is {pause} transcribing from one channel .
But there could be problems , right ? with that .
there are a lot of words that are so reduced phonetically that make sense when you know what the person was saying before .
Given all of the effort that is going on here in transcribing why do we have I B M doing it ?
Why not just do it all ourselves ?
and , um , here 's , uh , a {disfmarker} a , uh , collaborating institution that 's volunteered to do it .
uh , some point ago we thought that uh , it {disfmarker} " boy , we 'd really have to ramp up to do that " ,
So , that was a contribution they could make . Uh in terms of time , money , you know ?
So , um , Liz , with {disfmarker} with the SRI recognizer , {comment} can it make use of some time marks ?
um , I mean , for the SRI front - end , we really need to chop things up into pieces that are f not too huge .
Um , but second of all , uh {disfmarker} in general because some of these channels ,
sorry , some of the segments have a lot of cross - talk .
Um , it 's good to get sort of short segments if you 're gonna do recognition ,
especially forced alignment .
So we have to sort of normalize {comment} the front - end and so forth , and have these small segments .
So we 've taken that and chopped it into pieces based always on your {disfmarker} your , um , cuts that you made on the mixed signal .
And so that every {disfmarker} every speaker has the same cuts .
Um , the problem is if we have no time marks ,
then for forced alignment we actually don't know where {disfmarker} you know , in the signal the transcriber heard that word .
Th - but there 's going to be a real problem ,
uh , even if we chop up based on speech silence these , uh , the transcripts from I B M , we don't actually know where the words were ,
which segment they belonged to .
Now wasn't {disfmarker} I thought that one of the proposals was that IBM was going to do an initial forced alignment ,
I {disfmarker} I think that they are ,
and so we {disfmarker} we have to have a dialogue with them about it .
And the other one is , um , uh , is there some good use that we can make of the transcribers to do other things ?
but how do we step out the recorded meetings ?
Right , so I think we talking about three level {disfmarker} three things .
One {disfmarker} one was uh , we had s had some discussion in the past about some very high level labelings ,
types of overlaps , and so forth that {disfmarker} that someone could do .
Second was , uh , somewhat lower level
just doing these more precise timings .
And the third one is {disfmarker} is , uh , just a completely wild hair brained idea that I have
which is that , um , if , uh {disfmarker} if we have time and people are able to do it , to take some subset of the data and do some very fine grained analysis of the speech .
For instance , uh , marking in some overlapping {disfmarker}
potentially overlapping fashion , uh , the value of , uh , ar articulatory features .
and then this would give some more ground work for people who were building statistical models that allowed for overlapping changes , different timing changes as opposed to just " click ,
No , I think {disfmarker} I think it 's {disfmarker} for {disfmarker} for {disfmarker} for that purpose I 'm just viewing meetings as being a {disfmarker} a neat way to get people talking naturally .
And then you have i and then {disfmarker} and then it 's natural in all senses ,
in the sense that you have microphones that are at a distance that you know , one might have ,
and you have the close mikes ,
and you have people talking naturally .
And the overlap is just indicative of the fact that people are talking naturally ,
I guess I wanted to , um , sort of make a pitch for trying to collect more meetings .
because I think it 'd be good if {disfmarker} if we can get a few different sort of non - internal types of meetings
Um , the other thing is that {disfmarker} there was a number of things at the transcription side that , um , transcribers can do , like dialogue act tagging ,
disfluency tagging ,
um , things that are in the speech that are actually something we 're y {comment} working on for language modeling .
which is , um , uh , uh , Jonathan Fiscus expressed primar uh y a major interest in having meetings which were all English speakers .
so I think , you know , if he 's {disfmarker} if he 's thinking in terms of recognition kind of technology I {disfmarker} I {disfmarker} I think he would probably want , uh {vocalsound} American English ,
All I meant is just that as sort of {disfmarker} as this pipeline of research is going on we 're also experimenting with different ASR , uh , techniques .
but that , um , it would be helpful if I can stay in the loop somehow with , um , people who are doing any kind of post - processing ,
So the problem is like , uh , on the microphone of somebody who 's not talking they 're picking up signals from other people {comment} and that 's {vocalsound} causing problems ?
R right , although if they 're not talking , using the {disfmarker} the inhouse transcriptions , were sort of O K
Well we try to find as close of start and end time of {disfmarker} as we can to the speech from an individual speaker ,
for the forced alignment which is just to give us the time boundaries ,
because from those time boundaries then the plan is to compute prosodic features .
because then we {disfmarker} we 're more guaranteed that the recognizer will {disfmarker}
And the sort of more space you have that isn't the thing you 're trying to align the more errors we have .
uh , because of the fact that there 's enough acoustic signal there t for the recognizer to {disfmarker} to eat , {vocalsound} as part of a word .
but we probably will have to do something like that in addition .
because that {disfmarker} that 'll really help us .
